{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVKkUziSUImW"
      },
      "source": [
        "# Mount google drive to Google Colab\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGFncP6OpLo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8569e3f0-3bc6-4560-d1db-9d97e1c8105e"
      },
      "source": [
        "\"\"\" Cell 1 \"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFHRwoxsUImf"
      },
      "source": [
        "# Install necessary packages\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqqtzulciHxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1671e88c-caf5-42ec-97c2-80c213e5b5dc"
      },
      "source": [
        "pip install git+https://github.com/rcmalli/keras-vggface.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
            "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-je2y3t93\n",
            "  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-je2y3t93\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (3.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (7.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (2.6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (3.13)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-vggface==0.6) (1.5.2)\n",
            "Building wheels for collected packages: keras-vggface\n",
            "  Building wheel for keras-vggface (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-vggface: filename=keras_vggface-0.6-py3-none-any.whl size=8325 sha256=cf1799682e3d6068ff088e30c36de778df3f1950b99f23353da2acd089d051e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ofi9zaie/wheels/08/df/86/0225d44647ab2256dbf1e006823288fe9cc86367a056e6ea2c\n",
            "Successfully built keras-vggface\n",
            "Installing collected packages: keras-vggface\n",
            "Successfully installed keras-vggface-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ox3_TariaTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af8ffec-b01c-4d8a-a279-3e5155358bd9"
      },
      "source": [
        "pip show keras-vggface"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: keras-vggface\n",
            "Version: 0.6\n",
            "Summary: VGGFace implementation with Keras framework\n",
            "Home-page: https://github.com/rcmalli/keras-vggface\n",
            "Author: Refik Can MALLI\n",
            "Author-email: mallir@itu.edu.tr\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: h5py, pillow, pyyaml, keras, numpy, six, scipy\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJVRNrz6idMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd96b4e-a88d-43e5-f4b2-1232833f4e58"
      },
      "source": [
        "!pip install keras_vggface\n",
        "!pip3 install keras_applications\n",
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_vggface in /usr/local/lib/python3.7/dist-packages (0.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras_vggface) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_vggface) (3.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from keras_vggface) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_vggface) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras_vggface) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras_vggface) (3.13)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras_vggface) (2.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras_vggface) (1.5.2)\n",
            "Collecting keras_applications\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.5.2)\n",
            "Installing collected packages: keras-applications\n",
            "Successfully installed keras-applications-1.0.8\n",
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.6.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.19.5)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzxqu9h1kXsA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b94da398-930b-4dea-853b-b676a7d42fa7"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBh_8dmIl-UJ"
      },
      "source": [
        "\"\"\" Cell 6 for changing directory\"\"\"\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/EA_HAM_WS_2'\n",
        "\n",
        "# change directory to our base directory\n",
        "os.chdir(base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWPiZgA3FsTG"
      },
      "source": [
        "\"\"\" Cell 7 for importing packages necessary\"\"\"\n",
        "\n",
        "# import necessary modules for working\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from PIL import Image, ImageOps\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycPLsebPUImk"
      },
      "source": [
        "\n",
        "## 1. Extract faces (Region of Interest) from given data\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "    - use MTCNN detector to detect the faces and then store the face coordinates\n",
        "    - extract the bounding box of a face as an image\n",
        "    - save the cropped image in a folder with respective names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4zccBneLpwU"
      },
      "source": [
        "\"\"\" Cell 8 for face extraction from our image\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" Use detector to detect faces and find the face region \"\"\"\n",
        "\n",
        "# create the detector, using default weights\n",
        "detector = MTCNN()\n",
        "\n",
        "def extract_face(filename, required_size=(197, 197), detector = detector):\n",
        "    # load image from file\n",
        "    pixels = plt.imread(filename)\n",
        "    # detect faces in the image\n",
        "    faces = detector.detect_faces(pixels)\n",
        "    print(\"This is faces now:\\n\", faces)\n",
        "    # extract the bounding box from the first face\n",
        "    if len(faces) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        x1, y1, width, height = faces[0]['box']\n",
        "        x2, y2 = x1 + width, y1 + height\n",
        "        \n",
        "        # extract the face\n",
        "        face = pixels[y1:y2, x1:x2]\n",
        "        # resize pixels to the model size\n",
        "        img = Image.fromarray(face)\n",
        "\n",
        "        # ImageOps.exif_transpose makes sure that there is no orientation issues after resizing\n",
        "        img = ImageOps.exif_transpose(img.resize(required_size))\n",
        "        return img\n",
        "\n",
        "\"\"\" Demo Snippet \"\"\"\n",
        "\n",
        "test_extract_face = extract_face('/content/drive/MyDrive/EA_HAM_WS_2/team_images/captain/ca_1.jpg')\n",
        "plt.imshow(test_extract_face)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ptg_i4qliJ6"
      },
      "source": [
        "\"\"\" Cell 9 just a demo how for loops work in Cell 10\"\"\"\n",
        "\n",
        "\n",
        "# Read Image\n",
        "train_folders_path = 'team_images/*/'\n",
        "all_images = glob(train_folders_path + \"*.jpg\")\n",
        "all_images += glob(train_folders_path + \"*.png\")\n",
        "all_images += glob(train_folders_path + \"*.jpeg\")\n",
        "all_images += glob(train_folders_path + \"*.JPG\")\n",
        "all_images += glob(train_folders_path + \"*.PNG\")\n",
        "all_images += glob(train_folders_path + \"*.JPEG\")\n",
        "all_images.sort()\n",
        "\n",
        "print(all_images)\n",
        "\n",
        "# for larger image size, instead of default size\n",
        "plt.rcParams[\"figure.figsize\"] = (15,7)\n",
        "\n",
        "# to display images in the form of 2x6 matrix\n",
        "f, axarr = plt.subplots(2,6)\n",
        "i, j = 0, 0\n",
        "\n",
        "\"\"\" Demo Snippet \"\"\"\n",
        "\n",
        "for im in all_images:\n",
        "    cropped_im = extract_face(im)\n",
        "    if cropped_im == 0:\n",
        "        continue\n",
        "    else:\n",
        "        axarr[i,j].imshow(cropped_im)\n",
        "        j+=1\n",
        "        if j == 6:\n",
        "          i, j = 1, 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjB_fs4htWQx"
      },
      "source": [
        "\"\"\" Cell 10 as you've seen how above loops work, now use same code but instead of showing copy the images\n",
        "    from given data to 'face_data' folder after creating it \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" Save the images in a folder \"\"\"\n",
        "\n",
        "# make a directory to store cropped images\n",
        "if not os.path.exists('face_data'):\n",
        "    os.mkdir('face_data/')\n",
        "\n",
        "root_dir = 'face_data/'\n",
        "\n",
        "for im in all_images:\n",
        "    # print(im)\n",
        "    class_dir = root_dir + im.split('/')[-2]\n",
        "    if not os.path.exists(class_dir):\n",
        "        os.mkdir(class_dir)\n",
        "    cropped_im = extract_face(im)\n",
        "    if cropped_im == 0:\n",
        "        continue\n",
        "    else:\n",
        "      if not os.path.exists(class_dir +'/'+im.split('/')[-1]):\n",
        "        cropped_im.save(class_dir +'/'+im.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmVzP9_7surP"
      },
      "source": [
        "## 2. Prepare data for the model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "    - find number of people (classes) in the data\n",
        "    - assign numbers for easier classification of the people\n",
        "    - divide the data into train and validation sets for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWzMJAf8s14r"
      },
      "source": [
        "\"\"\" Cell 11 just try to get names of persons in 'retour variable' \"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "retour=[]\n",
        "\n",
        "# remember root_dir here is face_data\n",
        "for (root,dirs,files) in os.walk(root_dir):\n",
        "    for f in files:\n",
        "        if (f.endswith(\"jpg\")| f.endswith(\"JPG\") |f.endswith(\"jpeg\")| f.endswith(\"JPEG\") | f.endswith(\"PNG\") | f.endswith(\"png\")):\n",
        "            r=root.split('/')\n",
        "            lr=len(r)\n",
        "            retour.append((f,r[lr-1],root))\n",
        "\n",
        "print(\"== Found %d items \"%len(retour))\n",
        "print(root)\n",
        "print(r)\n",
        "print(f)\n",
        "print(retour)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PwVsbH6tW_k"
      },
      "source": [
        "\"\"\" Cell 12 give each person a unique code(here just numbers from 0 to n-1)\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" find total classes(here, number of people) in the given data set and assign unique indices to each class \"\"\"\n",
        "idx = {}\n",
        "\n",
        "for i in retour:\n",
        "    if i[1] not in idx:\n",
        "        idx[i[1]]=len(idx)\n",
        "\n",
        "print(\"== Found %d classes\"% len(idx))\n",
        "print(i)\n",
        "print(i[1])\n",
        "print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chB6PqDIuXZX"
      },
      "source": [
        "\"\"\" Cell 13 convert the idx to a csv file (a widely used file format for data-analysis, ML, etc)\"\"\"\n",
        "\n",
        "\n",
        "# store all the classes and their values in a CSV file\n",
        "\n",
        "df = pd.DataFrame.from_dict(idx, orient='index')\n",
        "print(df)\n",
        "df.to_csv('idx.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIo-oiP3thJ7"
      },
      "source": [
        "\"\"\" Cell 14 \"\"\"\n",
        "\n",
        "# make empty train and val directories\n",
        "os.mkdir('train')\n",
        "os.mkdir('val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEB2I3Z6tWzB"
      },
      "source": [
        "\n",
        "\"\"\" Cell 15 make a folder for each person in train and val folders\"\"\"\n",
        "\n",
        "# create train and val folders for each class\n",
        "for i in idx.keys():\n",
        "    print(i)\n",
        "    os.mkdir('train/'+i)\n",
        "    os.mkdir('val/'+i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzNk-4eBwQSf"
      },
      "source": [
        "\"\"\" Cell 16 demo for what we do in cell 17, just randomly sample our 'face_data' into train and val based on 70-30 splitting rule \"\"\"\n",
        "\n",
        "# check if there is any out of index before filling randomly 4 in train and 2 in val from given 6 images of data\n",
        "from random import sample, choice\n",
        "\n",
        "for i in idx.keys():\n",
        "    print(root_dir+i)\n",
        "    for (root,dirs,files) in os.walk(root_dir+'/'+i):\n",
        "        imgs = sample(files, 6)\n",
        "        print(imgs)\n",
        "        for j in range(4):\n",
        "            print(root_dir+i+'/'+imgs[j], 'train/'+i+'/'+imgs[j])\n",
        "        for j in range(2):\n",
        "            print(root_dir+i+'/'+imgs[j+4], 'val/'+i+'/'+imgs[j+4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cJXiMDjv9BI"
      },
      "source": [
        "\"\"\" Cell 17 now as you saw how the above cell loops works, instead of printing COPY the images from face_data to train and val \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" divide the data by filling randomly 4 in train and 2 in val from given 6 images of data \"\"\"\n",
        "\n",
        "import shutil\n",
        "from random import sample, choice\n",
        "\n",
        "for i in idx.keys():\n",
        "    for (root,dirs,files) in os.walk(root_dir+'/'+i):\n",
        "        imgs = sample(files, 6)\n",
        "        print(imgs)\n",
        "        for j in range(4):\n",
        "            shutil.copy(root_dir+i+'/'+imgs[j], 'train/'+i+'/'+imgs[j])\n",
        "        for j in range(2):\n",
        "            shutil.copy(root_dir+i+'/'+imgs[j+4], 'val/'+i+'/'+imgs[j+4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBg8yYbFUIm5"
      },
      "source": [
        "## 3. Train the model on data\n",
        "---\n",
        "\n",
        "\n",
        "    - read the image and resize it to required architecture input-size\n",
        "    - define a baseline model using ResNet architecture\n",
        "    - train the model on the train data and cross-validate with validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wx6H12IvIfX"
      },
      "source": [
        "\"\"\" Cell 18 \"\"\"\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "from random import choice, sample\n",
        "from glob import glob\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Flatten\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRMs0aTrNM2_"
      },
      "source": [
        "\"\"\" Cell 19 randomly sample images to get Batches of data to feed model\"\"\"\n",
        "\n",
        "classes = sample(idx.keys(),2)\n",
        "print(sample(idx.keys(),2))\n",
        "batch = []\n",
        "\n",
        "for i in classes:\n",
        "    print(i)\n",
        "    for (r,dirs,files) in os.walk('train'+'/'+i):\n",
        "        print(r,dirs,files)\n",
        "        batch.append('train'+'/'+i+'/'+choice(files))\n",
        "        print(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcmbF-GZscgT"
      },
      "source": [
        "\"\"\" Cell 20 \n",
        "    - process input so it satisfies necessary conditions for our model\n",
        "    - generate batches of data\n",
        "    - make a base_line model to start training\n",
        "    - train the baseline_model with necessary arguments set to heuristic values\n",
        "\"\"\"\n",
        "\n",
        "# training the model\n",
        "\n",
        "def read_img(path):\n",
        "    img = image.load_img(path, target_size=(197, 197))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "\n",
        "def gen(root, batch_size=2, idx = idx):\n",
        "    while True:\n",
        "        batch = []\n",
        "        labels = []\n",
        "        classes = sample(idx.keys(), len(idx.keys()))\n",
        "        \n",
        "        for i in classes:\n",
        "            for (r,dirs,files) in os.walk(root+'/'+i):\n",
        "                batch.append(root+'/'+i+'/'+choice(files))\n",
        "\n",
        "        labels = np.zeros((batch_size, len(idx.keys())))\n",
        "        for i in range(len(idx.keys())):\n",
        "            labels[i, idx[classes[i]]] = 1\n",
        "        X = np.array([read_img(x) for x in batch])\n",
        "\n",
        "        yield X, labels\n",
        "\n",
        "def baseline_model(classes = 2):\n",
        "    base_model = VGGFace(model='resnet50', include_top=False, input_shape=(197, 197, 3))\n",
        "    last_layer = base_model.get_layer('avg_pool').output\n",
        "    y = Flatten(name='flatten')(last_layer)\n",
        "\n",
        "    for x in base_model.layers[:-3]:\n",
        "        x.trainable = False\n",
        "    for x in base_model.layers[-3:]:   # train only last 2 layers of the model, which are last_layer and Flatten Layer we defined above\n",
        "        x.trainable = True\n",
        "\n",
        "    out = Dense(classes, activation=\"softmax\")(y)  # (Dense from keras) here our total predictions are 2 classes\n",
        "\n",
        "    model = Model(base_model.input, out) # Model from keras.models\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00002))\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model():\n",
        "    file_path = \"vgg_face.h5\"\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=10, verbose=1)\n",
        "    es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.0001, patience=20, verbose=1)\n",
        "    callbacks_list = [checkpoint, reduce_on_plateau, es]\n",
        "\n",
        "    history = model1.fit_generator(gen('train', batch_size=2),\n",
        "                                  use_multiprocessing=False,\n",
        "                                  validation_data=gen('val', batch_size=2), \n",
        "                                  epochs=3, verbose=1, max_queue_size = 10,  \n",
        "                                  workers=1, callbacks=callbacks_list, \n",
        "                                  steps_per_epoch=100, validation_steps=50)\n",
        "\n",
        "classes = 2\n",
        "model1 = baseline_model(classes)\n",
        "\n",
        "train_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEVm8GPk2GXd"
      },
      "source": [
        "\"\"\" Cell 21 save the trained model to h5 file\"\"\"\n",
        "\n",
        "\"\"\" save the trained model in form of .h5 file (type of Hierarchical Data Format) for ease of storing weights and model configuration\"\"\"\n",
        "\n",
        "model1.save(\"face_recognition_model.h5\")\n",
        "# model1.save(\"face_recognition_model_pretrained.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Oj7rCx7S4L"
      },
      "source": [
        "\"\"\" Cell 22 a code snippet to visualise the model you just trained \"\"\"\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "model = load_model(\"face_recognition_model.h5\")\n",
        "\n",
        "# visualize model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp7Qz0hu0hki"
      },
      "source": [
        "## 4. Predict the classes (faces)\n",
        "---\n",
        "\n",
        "\n",
        "    - use the saved weights .h5 file to create a model for testing\n",
        "    - load the numbers corresponding to our classes from csv file\n",
        "    - predict the faces in groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVTxirWLFcX3"
      },
      "source": [
        "\"\"\" Cell 23 import modules and define where computer should look for your predicition images faces\"\"\"\n",
        "\n",
        "# from here we can use the saved model to recognise faces\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "test_imgs = 'test'\n",
        "\n",
        "im_path = glob(test_imgs+'/*.jpg')\n",
        "print(im_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkvDSDTlGbVG"
      },
      "source": [
        "\"\"\" Cell 24 convert csv file back to a list for coding later on\"\"\"\n",
        "\n",
        "idx = pd.read_csv('idx.csv')\n",
        "idx = idx.values.tolist()\n",
        "print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w17BQw9vGLYP"
      },
      "source": [
        "\"\"\" Cell 25 load the trained model into 'model' variable \"\"\"\n",
        "\n",
        "\"\"\" Use our trained model for testing \"\"\"\n",
        "\n",
        "model_path = 'face_recognition_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "classes = 2\n",
        "att = np.zeros(classes) # your classes\n",
        "print(att)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhso7JvxJ2hY"
      },
      "source": [
        "\"\"\" Cell 26 write a function to \"\"\"\n",
        "\n",
        "# create the detector, using default weights\n",
        "detector = MTCNN()\n",
        "\n",
        "\"\"\" Now lets use the model to find faces in a group \"\"\"\n",
        "\n",
        "def recognise_person(filename, required_size=(197, 197), idx = idx, att = att):\n",
        "    # load image from file\n",
        "    pixels = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB) # if you remember BGR is standard cv2 format, so convert to RGB channel space\n",
        "    pixels = cv2.copyMakeBorder(pixels, 100, 100, 100, 100, cv2.BORDER_CONSTANT)\n",
        "    pixels_l = cv2.cvtColor(pixels, cv2.COLOR_RGB2BGR)\n",
        "    # detect faces in the image\n",
        "    faces = detector.detect_faces(pixels)\n",
        "    print(faces)\n",
        "    for f in range(len(faces)):\n",
        "        # extract the bounding box from the first face\n",
        "        if(faces[f]['confidence']<0.9): # extract only if model has more than 90 % confidence that its a face\n",
        "            print(faces[f]['confidence'])\n",
        "            continue\n",
        "        x1, y1, width, height = faces[f]['box']  # from here this is same as extract_face as we defined above\n",
        "        x2, y2 = x1 + width, y1 + height\n",
        "        # extract the face\n",
        "        face = pixels[y1:y2, x1:x2]\n",
        "        \n",
        "        # resize pixels to the model size\n",
        "        image = Image.fromarray(face)\n",
        "        image = image.resize(required_size)\n",
        "\n",
        "        face = np.asarray(image, dtype = np.float64)\n",
        "        face = np.copy(face)\n",
        "        face = preprocess_input(face, version=2)\n",
        "        face = np.reshape(face, (1, 197, 197, 3))\n",
        "\n",
        "        \"\"\" Predict which class (person) the face belongs to using our model \"\"\"\n",
        "        Y = model.predict(face) \n",
        "\n",
        "        att[np.argmax(Y)] = 1  # among all our persons names only take the unique-code whose final probability from model is highest\n",
        "\n",
        "        # draw a rectangle around a person face\n",
        "        cv2.rectangle(pixels_l, (x1, y1), (x2, y2), (66,245,224), 2)\n",
        "        \n",
        "        # put person name at top of the face\n",
        "        cv2.putText(pixels_l, str(idx[np.argmax(Y)][0]), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.75 , (66,245,224), 2, cv2.LINE_AA)\n",
        "    \n",
        "    cv2.imwrite(test_imgs+'/labeled_'+filename.split('/')[-1], pixels_l)\n",
        "\n",
        "    return \n",
        "\n",
        "for im in im_path:\n",
        "    recognise_person(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT3XYYpaDMDU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xme5d4s8COcw"
      },
      "source": [
        "###Yayy!! you've successfully completed training and testing a face recognition model\n",
        "Remember, AI is not just about coding in some fancy programming langauage, its about how well can we apply concepts to come up with better solutions. \n",
        "Sometimes ML or AI is more of an art rather than science! \n",
        "\n",
        "**Just keep learning!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aClU5vFPKAEN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}